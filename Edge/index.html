<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Edge - STAR ATLAS</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Edge";
    var mkdocs_page_input_path = "Edge.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> STAR ATLAS</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Dynamics</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Edge</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#edge">Edge</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#20190629-ai-aided-high-speed-navigation-for-drone">20190629 AI-aided high speed navigation for drone</a></li>
        
            <li><a class="toctree-l4" href="#20190628-deepmind">20190628  DeepMind 在多智能体强化学习的新进展</a></li>
        
            <li><a class="toctree-l4" href="#20190626-robobee-x-wing">20190626 哈佛发布新一代最轻飞行机器人 RoboBee X-Wing</a></li>
        
            <li><a class="toctree-l4" href="#20190625">20190625 第四代树莓派发布</a></li>
        
            <li><a class="toctree-l4" href="#20190616">20190616 世界首个触觉传输遥操作机器手</a></li>
        
            <li><a class="toctree-l4" href="#20190615-40">20190615 基于视觉控制，40美元开源无传感器机械臂</a></li>
        
            <li><a class="toctree-l4" href="#20190614-google-weight-agnostic-neural-networks">20190614 Google: Weight Agnostic Neural Networks</a></li>
        
            <li><a class="toctree-l4" href="#20190602-open-source-deep-learning-powered-visual-navigation-engine-for-uavs">20190602 Open Source Deep Learning-powered Visual Navigation Engine for UAVs</a></li>
        
            <li><a class="toctree-l4" href="#20190601-closing-the-sim-to-real-loop-adapting-simulation-randomization-with-real-world-experience">20190601 Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</a></li>
        
            <li><a class="toctree-l4" href="#20190526-iithyqreal">20190526 IIT最新一代液压四足机器人(HyQReal)发布</a></li>
        
            <li><a class="toctree-l4" href="#20190525-stanford-doggo">20190525 Stanford Doggo全开源四足机器人</a></li>
        
            <li><a class="toctree-l4" href="#20190513">20190513 普渡大学蜂鸟机器人项目</a></li>
        
            <li><a class="toctree-l4" href="#20190512">20190512 微软命令行工具</a></li>
        
            <li><a class="toctree-l4" href="#20190511-siammask">20190511 SiamMask 快速在线对象跟踪和分割</a></li>
        
            <li><a class="toctree-l4" href="#20190506-cassie">20190506 深度强化学习训练双足机器人Cassie</a></li>
        
            <li><a class="toctree-l4" href="#20190505-real-time-3d-human-body-motion-prediction">20190505 Real-time 3D Human Body Motion Prediction</a></li>
        
            <li><a class="toctree-l4" href="#20190413">20190413 人类史上首张黑洞照片发布</a></li>
        
            <li><a class="toctree-l4" href="#20190407-handle">20190407 波士顿动力定位于物流应用的Handle机器人硬件平台技术浅析</a></li>
        
            <li><a class="toctree-l4" href="#20190321-robotic-collectives-inspired-by-biological-cells">20190321 Robotic collectives inspired by biological cells</a></li>
        
            <li><a class="toctree-l4" href="#20190319-nvidia-jetson-nano">20190319 NVIDIA 推出 Jetson Nano 人工智能计算机</a></li>
        
            <li><a class="toctree-l4" href="#20190309">20190309 驯服自动驾驶的长尾挑战</a></li>
        
            <li><a class="toctree-l4" href="#20190305-mit-mini-cheetah-bot">20190305 MIT Mini Cheetah Bot</a></li>
        
            <li><a class="toctree-l4" href="#20190304-mit-tech-review-2019">20190304 MIT Tech Review 2019年“全球十大突破性技术”</a></li>
        
            <li><a class="toctree-l4" href="#20190228-2018">20190228 中国十大2018年度科学进展</a></li>
        
            <li><a class="toctree-l4" href="#20190221-learning-agile-and-dynamic-motor-skills-for-legged-robots">20190221 Learning agile and dynamic motor skills for legged robots</a></li>
        
            <li><a class="toctree-l4" href="#20190219-task-agnostic-self-modelling-machines">20190219 Task Agnostic Self-modelling Machines</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../Strategy/">Strategy</a>
                </li>
                <li class="">
                    
    <a class="" href="../Perspective/">Perspective</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">AI</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../AI/">AI Map</a>
                </li>
                <li class="">
                    
    <a class="" href="../AI Index 2018 Annual Report/">AI Index 2018 Annual Report</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Self-driving</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Self-driving/">Self-driving Map</a>
                </li>
                <li class="">
                    
    <a class="" href="../pdf/selfdriving_new.pdf">Self-driving v2 (PDF,Aminer)</a>
                </li>
                <li class="">
                    
    <a class="" href="../pdf/selfdriving_basic.pdf">Self-driving v1 (PDF,Aminer)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Flying Car</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Flying car/">Flying Car Map</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Blockchain</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../pdf/blockchain_public.pdf">Blockchain Overview (PDF,Aminer)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Robotics</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Robotics/">Robotics Collections</a>
                </li>
                <li class="">
                    
    <a class="" href="../howToRobotics/">How to Study Robotics</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Gadgets</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Tools/">Tools</a>
                </li>
                <li class="">
                    
    <a class="" href="../Lifestyle/">Lifestyle</a>
                </li>
                <li class="">
                    
    <a class="" href="../Git/">Git</a>
                </li>
                <li class="">
                    
    <a class="" href="../Blog/">Blog Deployment</a>
                </li>
                <li class="">
                    
    <a class="" href="../howToSearch/">How to Search</a>
                </li>
                <li class="">
                    
    <a class="" href="../KnowledgeSystem/">Knowledge System</a>
                </li>
                <li class="">
                    
    <a class="" href="../thinkTank/">Think Tanks</a>
                </li>
                <li class="">
                    
    <a class="" href="../MgtFinEco/">MgtFinEco</a>
                </li>
                <li class="">
                    
    <a class="" href="../howToBuyFunds/">Funds</a>
                </li>
                <li class="">
                    
    <a class="" href="../Hangzhou/">How to Hangzhou</a>
                </li>
                <li class="">
                    
    <a class="" href="../howToBuyCars/">How to Cars</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">STAR ATLAS</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Dynamics &raquo;</li>
        
      
    
    <li>Edge</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="edge">Edge</h1>
<p>Collected by Jianan, 2019. 本章节会实时收录全球科技动态。部分文字从原文直接引用，如有侵权，立即删除。本文仅供个人参考学习，不作商业用途。</p>
<div class="toc">
<ul>
<li><a href="#edge">Edge</a><ul>
<li><a href="#20190629-ai-aided-high-speed-navigation-for-drone">20190629 AI-aided high speed navigation for drone</a></li>
<li><a href="#20190628-deepmind">20190628  DeepMind 在多智能体强化学习的新进展</a></li>
<li><a href="#20190626-robobee-x-wing">20190626 哈佛发布新一代最轻飞行机器人 RoboBee X-Wing</a></li>
<li><a href="#20190625">20190625 第四代树莓派发布</a></li>
<li><a href="#20190616">20190616 世界首个触觉传输遥操作机器手</a></li>
<li><a href="#20190615-40">20190615 基于视觉控制，40美元开源无传感器机械臂</a></li>
<li><a href="#20190614-google-weight-agnostic-neural-networks">20190614 Google: Weight Agnostic Neural Networks</a></li>
<li><a href="#20190602-open-source-deep-learning-powered-visual-navigation-engine-for-uavs">20190602 Open Source Deep Learning-powered Visual Navigation Engine for UAVs</a></li>
<li><a href="#20190601-closing-the-sim-to-real-loop-adapting-simulation-randomization-with-real-world-experience">20190601 Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</a></li>
<li><a href="#20190526-iithyqreal">20190526 IIT最新一代液压四足机器人(HyQReal)发布</a></li>
<li><a href="#20190525-stanford-doggo">20190525 Stanford Doggo全开源四足机器人</a></li>
<li><a href="#20190513">20190513 普渡大学蜂鸟机器人项目</a></li>
<li><a href="#20190512">20190512 微软命令行工具</a></li>
<li><a href="#20190511-siammask">20190511 SiamMask 快速在线对象跟踪和分割</a></li>
<li><a href="#20190506-cassie">20190506 深度强化学习训练双足机器人Cassie</a></li>
<li><a href="#20190505-real-time-3d-human-body-motion-prediction">20190505 Real-time 3D Human Body Motion Prediction</a></li>
<li><a href="#20190413">20190413 人类史上首张黑洞照片发布</a></li>
<li><a href="#20190407-handle">20190407 波士顿动力定位于物流应用的Handle机器人硬件平台技术浅析</a></li>
<li><a href="#20190321-robotic-collectives-inspired-by-biological-cells">20190321 Robotic collectives inspired by biological cells</a></li>
<li><a href="#20190319-nvidia-jetson-nano">20190319 NVIDIA 推出 Jetson Nano 人工智能计算机</a></li>
<li><a href="#20190309">20190309 驯服自动驾驶的长尾挑战</a></li>
<li><a href="#20190305-mit-mini-cheetah-bot">20190305 MIT Mini Cheetah Bot</a></li>
<li><a href="#20190304-mit-tech-review-2019">20190304 MIT Tech Review 2019年“全球十大突破性技术”</a></li>
<li><a href="#20190228-2018">20190228 中国十大2018年度科学进展</a></li>
<li><a href="#20190221-learning-agile-and-dynamic-motor-skills-for-legged-robots">20190221 Learning agile and dynamic motor skills for legged robots</a></li>
<li><a href="#20190219-task-agnostic-self-modelling-machines">20190219 Task Agnostic Self-modelling Machines</a></li>
</ul>
</li>
</ul>
</div>
<h3 id="20190629-ai-aided-high-speed-navigation-for-drone">20190629 <a href="https://spectrum.ieee.org/tech-talk/robotics/drones/to-fly-solo-racing-drones-have-need-for-ai-speed-training">AI-aided high speed navigation for drone</a></h3>
<p>U.S. military and defense industry are betting on autonomous drone racing as the next frontier for developing AI so that it can handle high-speed navigation within tight spaces without human intervention.</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190628-deepmind">20190628  <a href="https://mp.weixin.qq.com/s/3TFQR2uVtTwFddKf6S9SnA">DeepMind 在多智能体强化学习的新进展</a></h3>
<p>智能体在多玩家电子游戏中掌握策略、理解战术以及进行团队协作是人工智能研究领域的重大挑战。我们发表在「科学」杂志上的最新论文「Human-level performance in 3D multiplayer games with population-based reinforcement learning」中，展示了智能体在强化学习领域的最新进展，在「Quake III Arena」夺旗赛（CTF）中取得了与人类水平相当的性能。这是一个复杂的多智能体环境，也是第一人称多玩家的经典三维游戏之一。这些智能体成功地与 AI 队友和人类队友协作，表现出了很高的性能，即使在训练时其反应时间，表现也与人类相当。此外，我们还展示了如何能够成功地将这些方法从研究 CTF 环境中扩展到完整的「Quake III Arena」游戏中。</p>
<h3 id="20190626-robobee-x-wing">20190626 <a href="https://mp.weixin.qq.com/s/NILmGjak2efmmVlHPCTF_g">哈佛发布新一代最轻飞行机器人 RoboBee X-Wing</a></h3>
<p>在今天Nature杂志的封面文章中，由Robert J. Wood教授领导的哈佛大学微机器人实验室的研究人员展示了他们模拟昆虫制造的飞行机器人，这是一个四翼的蜜蜂机器人，他们称之为RoboBee X-Wing。重259毫克，比回形针还轻，翼展约3.5厘米。使用太阳能电池，只要有光源就能持续、不受束缚地飞行。</p>
<p><img alt="20190629154555" src="../fig/20190629154555.gif" /></p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190625">20190625 <a href="https://mp.weixin.qq.com/s/OclgWNG-t2-stmFIZbHo-w">第四代树莓派发布</a></h3>
<p><img alt="20190629155138" src="../fig/20190629155138.jpg" /></p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190616">20190616 <a href="https://mp.weixin.qq.com/s/bhhy9zXVlQ0LyHucJW1Z5w">世界首个触觉传输遥操作机器手</a></h3>
<p><img alt="20190616215632" src="../fig/20190616215632.gif" /></p>
<h3 id="20190615-40">20190615 <a href="https://mp.weixin.qq.com/s/dZM-KN6dg-WUMaufeg6tnQ">基于视觉控制，40美元开源无传感器机械臂</a></h3>
<ul>
<li>
<p>项目网站：https://craves.ai（含代码和数据）</p>
</li>
<li>
<p>论文地址：https://arxiv.org/abs/1812.00725</p>
<p><img alt="20190616215907" src="../fig/20190616215907.gif" /></p>
</li>
</ul>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190614-google-weight-agnostic-neural-networks">20190614 <a href="https://mp.weixin.qq.com/s/znwsjfczWNpT8wHjy2CYlA">Google: Weight Agnostic Neural Networks</a></h3>
<p>来自德国波恩-莱茵-锡格应用技术大学和谷歌大脑的一项新研究提出了一种神经网络架构搜索方法，这些网络可以在不进行显式权重训练的情况下执行各种任务。</p>
<p>在生物学中，早成性物种是指那些天生就有一些能力的幼生体。很多证据表明蜥蜴和蛇等动物天生就懂得逃避捕食者，鸭子在孵化后也能自己学会游泳和进食。相比之下，我们在训练智能体执行任务时，会选择一个典型的神经网络框架，并相信它有潜力为这个任务编码特定的策略。注意这里只是「有潜力」，我们还要学习权重参数，才能将这种潜力变化为能力。受到自然界早成行为及先天能力的启发，在这项工作中，研究者构建了一个能「自然」执行给定任务的神经网络。也就是说，找到一个先天的神经网络架构，然后只需要随机初始化的权重就能执行任务。研究者表示，这种不用学习参数的神经网络架构在强化学习与监督学习都有很好的表现。</p>
<p>其实在我们的理解中，如果我们想象神经网络架构提供的就是一个圈，那么常规学习权重就是找到一个最优「点」（或最优参数解）。但是对于不用学习权重的神经网络，它就相当于引入了一个非常强的归纳偏置，以至于，整个架构偏置到能直接解决某个问题。</p>
<p><img alt="20190616213911" src="../fig/20190616213911.jpg" /></p>
<p><img alt="20190616213923" src="../fig/20190616213923.gif" /></p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190602-open-source-deep-learning-powered-visual-navigation-engine-for-uavs">20190602 <a href="https://mp.weixin.qq.com/s/z5D7LXkwZ_ZWq7FA3mhD-Q">Open Source Deep Learning-powered Visual Navigation Engine for UAVs</a></h3>
<p>苏黎世联邦理工学院和博洛尼亚大学的研究人员最近发明了一个名叫PULP Dronet的纳米级无人机，仅重27g，可以说是目前重量最轻的无人机。这个微型无人机搭载顶尖的深度学习算法，可以在一个端到端的闭环视觉管道上运行。</p>
<p>这个无人机导航系统采用一个摄像头框架，并用最先进的CNN对其进行处理。随后，它决定如何纠正无人机的姿态，使其处于当前场景的中心。同样的CNN也识别出了障碍，如果无人机感觉到迫在眉睫的威胁，就停止它。“基本上，我们的无人机可以沿着一条街道（或类似的道路，例如走廊）行驶，并在遇到意外障碍时避免碰撞和刹车，”研究人员说：“与以前的口袋飞行机器人相比，我们的系统提供的真正飞跃是，实现自主导航所需的所有操作都是直接在机身上执行的，不需要人工操作人员，也不需要特别的基础设施（比如外部摄像机或信号），尤其是没有任何用于计算的远程基站（比如远程笔记本电脑）。”</p>
<p><img alt="20190602110434" src="../fig/20190602110434.gif" /></p>
<ul>
<li>
<p>参考来源：https://techxplore.com/news/2019-05-pulp-dronet-gram-nano-uav-insects.html</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/1905.04166.pdf</p>
</li>
<li>
<p>GitHub地址：https://github.com/pulp-platform/pulp-dronet</p>
</li>
</ul>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190601-closing-the-sim-to-real-loop-adapting-simulation-randomization-with-real-world-experience">20190601 Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</h3>
<p><a href="https://www.nvidia.com/en-us/research/robotics">Nvidia Research Group</a></p>
<p>来自西雅图NVIDIA器人研究实验室的研究人员正在展示一种新的概念强化学习方法，<strong>旨在通过仿真训练来提高机器人在现实世界中的表现。</strong></p>
<p>在他们的实验中，NVIDIA研究人员使用64个NVIDIA Tesla V100 GPU集群，使用cuDNN加速的TensorFlow深度学习框架，训练机器人完成两项任务：将一个挂钩放进一个洞，打开一个抽屉。对于场景的模拟，该团队使用了NVIDIA FleX物理引擎。</p>
<p><img alt="20190602105003" src="../fig/20190602105003.gif" /></p>
<p>机器人的整个学习过程大致是这样的，机器人首先在模拟环境中进行建模仿真，并且在虚拟环境中进行不断地训练，将测试得到的数据下载到机器人上，当在真实机器人上尝试学习任务时，系统准确观察它是如何失败的，并将失败的数据与模拟数据进行对比，将结果返回到学习框架进行优化模拟以获得更接近真实的模拟参数。</p>
<p><img alt="20190602104933" src="../fig/20190602104933.gif" /></p>
<p><img alt="20190602104946" src="../fig/20190602104946.gif" /></p>
<p>得益于深度学习框架的<strong>黑盒形态</strong>，虚拟环境的构建者可以<u>摆脱复杂的物理定律、具体的数学模型构建</u>，将这些虚拟环境的参数调整纳入到深度学习框架，经过不断地迭代后<strong>，</strong>系统能够识别出与现实世界中观察到的更接近的模拟参数，从而取得成功，进一步打通了虚拟与现实之间的隔阂。这为构建更为真实的虚拟环境和更具效率的机器人学习提供了另一种手段。</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190526-iithyqreal">20190526 <a href="https://mp.weixin.qq.com/s/qPa5BT98Bm_M7eBfM8N4CQ">IIT最新一代液压四足机器人(HyQReal)发布</a></h3>
<p><a href="https://www.iit.it">Istituto Italiano di Tecnologia </a>  意大利技术研究院</p>
<p><a href="https://zhuanlan.zhihu.com/p/66805417?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=62270998577152&amp;from=timeline&amp;s_r=0">知乎链接</a></p>
<p>5月23日，意大利技术研究院（iit）的动态腿足系统实验室（Dynamic Legged System，DLS） 发布了其新一代液压四足机器人：HyQReal，并放出了一段浮夸风溢出的机器人Demo——HyQReal拖拽小型客机，展现了基于液压原理的四足机器人的高负载能力。</p>
<p><img alt="20190526104537" src="../fig/20190526104537.jpg" /></p>
<p><strong>1、动力自主性(Power-autonomy)：</strong>这一点对于衡量液压机器人的性能是很重要的。简而言之，动力源外置（锂电池+电动液压泵）总会被别人诟病系统不够紧凑，不能展现真实的运动性能，实用性不够等等——HyQReal终于摆脱外接动力源的“大辫子（液压管道）”了，也意味着不用再给别人揪住缺乏动力自主性的“小辫子”，这是一个极大的进步！</p>
<p><strong>2、高输出能力(Strength)：</strong>在这个紧凑的几何尺寸下（1.3mx 0.67m x 0.9m），髋关节的侧摆能够达到165Nm的峰值扭矩，髋关节的前后摆腿达到200Nm的峰值扭矩，膝关节的前后摆腿达到225Nm的峰值扭矩，这个数值对比于电机驱动的大尺寸人形机器人也是非常优秀的——体现了无法替代的液压驱动优势。但130kg的自重还是有点超过作者的预计，未来会不会影响其动态运动性能还是未知数，因为腿足式机器人一直强调的是输出扭矩/质量密度和输出功率/质量密度，要把自身的重量考虑在内。但毕竟采用了动力源内置（锂电池+液压动力源）的设计，机器人的自身重量就很难降下来。</p>
<p><strong>3、坚固可靠性设计(RuggedDesign)：</strong>对于这点作者的理解是，因为采用了模块化、3D金属打印的液压驱动单元（ISA），一方面是极大提高了系统的可维护性与紧凑型，二是几乎消灭了裸露于外部的液压管道，这样就极大地提升了与外界物理交互的鲁棒性。</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190525-stanford-doggo">20190525 <a href="https://mp.weixin.qq.com/s/hJ9AMwgREPlIvDnqXg8PvA">Stanford Doggo全开源四足机器人</a></h3>
<p><img alt="20190526103212" src="../fig/20190526103212.gif" /></p>
<p>Stanford Doggo，它是一种准直接驱动（quasi-direct-drive）的四足机器人，具有很强的动态运动能力。该机器人能媲美或超过当前最优多足机器人的一般性能指标。且在垂直跳跃灵敏度上，即以平均垂直速度为指标，Stanford Doggo 能与表现最好的动物相媲美，并超过此前表现最好的机器人 22%。整体设计架构重点关注准直接驱动的设计方法。复现该机器人的硬件和软件都已经开源，只需要手工工具制造和组装就能完成，总成本低于 3000 美元。</p>
<ul>
<li>
<p>项目地址：<a href="https://github.com/Nate711/StanfordDoggoProject">https://github.com/Nate711/StanfordDoggoProject</a></p>
</li>
<li>
<p>CAD 设计图：<a href="https://a360.co/2OBxTbH">https://a360.co/2OBxTbH</a></p>
</li>
<li>
<p>论文地址：<a href="https://arxiv.org/abs/1905.04254">https://arxiv.org/abs/1905.04254</a></p>
</li>
</ul>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190513">20190513 <a href="https://mp.weixin.qq.com/s/mV4GB_ziUGQCh0eVQGVdKQ">普渡大学蜂鸟机器人项目</a></h3>
<p><img alt="img" src="../fig/20190513090130.gif" /></p>
<p>At Purdue University’s <a href="https://engineering.purdue.edu/~xdeng/">Bio-Robotics Lab</a>, <a href="https://engineering.purdue.edu/ME/People/ptProfile?id=57378">Xinyan Deng</a> </p>
<p>The code is available at (<a href="https://github.com/purdue-biorobotics/flappy">this https URL</a>). 6 pages, 10 figure, accepted at ICRA 2019</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190512">20190512 <a href="https://mp.weixin.qq.com/s/YYzvi4FXwwCK7Hk-TwZ8kA">微软命令行工具</a></h3>
<p>Windows Terminal是微软推出的新的命令行应用程序，提供多标签、分割窗口、快捷键、完整的Unicode字符支持等功能。支持PowerShell，Cmd，WSL（Windows的Linux子系统）和SSH等命令行程序，微软还给Windows Terminal加入很多细节功能，让它更美观。Windows Terminal使用的是基于DirectWrite/DirectX的GPU加速文本渲染引擎。这个新的引擎支持显示PC中存在的文本字符，意味着终端里的汉字、日文不再乱码，而且还能玩emoji表情。在微软展示的Demo中，你可以为自己的shell程序通过测试加入emoji表情提示。</p>
<p><a href="https://github.com/microsoft/Terminal">https://github.com/microsoft/Terminal</a></p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190511-siammask">20190511 <a href="https://zhuanlan.zhihu.com/p/58154634?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=675376553383432192&amp;wechatShare=1&amp;s_s_i=zSj7Ue3xiDKtWNoSF2LSKhrlfRqvgKjN7DXbqlEWl9g%3D&amp;s_r=0&amp;from=timeline&amp;isappinstalled=0">SiamMask 快速在线对象跟踪和分割</a></h3>
<p><a href="https://github.com/foolwood/SiamMask">https://github.com/foolwood/SiamMask</a></p>
<p><a href="https://arxiv.org/abs/1812.05050">https://arxiv.org/abs/1812.05050</a></p>
<p><img alt="img" src="../fig/v2-4efe88cf39f7d976a512a7e7345ea651_b.gif" /></p>
<p>In this paper we illustrate how to perform both <strong>visual object tracking</strong> and <strong>semi-supervised video object segmentation</strong>, in real-time, with a single simple approach. Our method, dubbed <strong>SiamMask</strong>, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is <a href="http://www.robots.ox.ac.uk/~qwang/SiamMask">this http URL</a>.</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190506-cassie">20190506 <a href="https://mp.weixin.qq.com/s/3k66lh5_bQCuFkamMEaTTQ">深度强化学习训练双足机器人Cassie</a></h3>
<p><a href="https://arxiv.org/abs/1903.09537">https://arxiv.org/abs/1903.09537</a></p>
<p><img alt="img" src="../fig/640.gif" /></p>
<p>深度学习在图像分类、图像分割、动作识别、语义理解、围棋、Flappy Bird、Dota等许多领域的高度非线性任务学习的问题上，取得了巨大的成就，但是将深度学习应用于真实世界机器人，还存在许多挑战和困难，例如需要依赖在虚拟环境下长时间的加速训练、虚拟环境模型与真实世界存在差异、机器人硬件的磨损等等。</p>
<p>加拿大大不列颠哥伦比亚大学计算机系和俄勒冈州立大学动力机器人实验室的研究人员对此进行了探索。为了让双足机器人学会在不同的速度下平稳地行走，研究人员提出一种简单而有效的方法，奖励函数在每一次迭代中可以重新定义，来学习新的策略，从虚拟环境下获取5-10K样本，然后通过这些少量样本将强化学习和有监督学习结合，学习机器人行走策略，并更成功转移到真实世界的机器人。</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190505-real-time-3d-human-body-motion-prediction">20190505 <a href="https://mp.weixin.qq.com/s/ZFXZnniR2fBnxong_kAksw">Real-time 3D Human Body Motion Prediction</a></h3>
<p>东京工业大学研究小组发布了一套格斗训练系统“FuturePose”，通过深度学习能预测 0.5 秒后对手的动作。通过从一个 RGB 相机捕获的图像中，从 30 fps（1帧= 1/30秒）图像中预测15帧后，即0.5秒后的动作，然后进行战斗训练。对战对手不同装束，而受训者可以戴 VR 头盔来同时观察对手的当前姿势和预测的0.5秒后的姿势。</p>
<p><img alt="img" src="../fig/cassie.gif" /></p>
<h3 id="20190413">20190413 <a href="https://mp.weixin.qq.com/s/zKNJBMqCNrF5GFPzQmEzfg">人类史上首张黑洞照片发布</a></h3>
<p>天文学家公布了人类史上首张黑洞照片。这颗黑洞就是M87星系中心的超大质量黑洞，它的质量是太阳的65亿倍，距离地球5500万光年。由事件视界望远镜（EHT）拍摄的黑洞照片。</p>
<p><img alt="img" src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAWR7KMTicXl4micou1JFQYicKLUZsf7zToaVS32wa8V7LofcsPInEiabNjfHFeCibX47XxKAEQYNMaIwA/640?wx_fmt=png" /></p>
<p><img alt="img" src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAWR7KMTicXl4micou1JFQYicKuzEc9CCicEic6xYoerZOvkGVXCClC5ZF4s2jPkLOkEQJfF4gcdmDbdgA/640?wx_fmt=png" /></p>
<h3 id="20190407-handle">20190407 <a href="https://zhuanlan.zhihu.com/p/60826728">波士顿动力定位于物流应用的Handle机器人硬件平台技术浅析</a></h3>
<p><img alt="v2-17cd6c99566b54361ab43752c7273310_hd" src="../fig/v2-17cd6c99566b54361ab43752c7273310_hd.jpg" /></p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190321-robotic-collectives-inspired-by-biological-cells">20190321 <a href="https://mp.weixin.qq.com/s/mcvnWRr0etf5w4tJFSQtew">Robotic collectives inspired by biological cells</a></h3>
<p><a href="https://www.nature.com/articles/d41586-019-00839-x">Nature 链接</a>， 论文<a href="../pdf/Particle robotics based on statistical mechanics of loosely coupled components.pdf"><code>本地下载</code></a></p>
<p>李曙光等人提出了一种机器人系统，它由许多松散耦合、随机移动、厘米级的组件组成。每个组件只能通过沿其半径摆动，通过伸展和收缩来移动。在这种振荡期间，组件的颜色代表它们的直径，绿色是最小的、蓝色是最大的；用于测试系统鲁棒性的故障组件用栗色表示。作者表明，他们的系统能够在躲避障碍物的同时，随着时间的推移，向环境信号(例如光源)表现出确定性的运动。</p>
<p><img alt="img" src="https://mmbiz.qpic.cn/mmbiz_jpg/UicQ7HgWiaUb0tcmXDKmYswp4jknLc0ial4icfoSC4L8mBB3bNiaVQLdZ0Ly9G30yEEvh7xmI0X8GKSdiczuib61CaxoQ/640?wx_fmt=jpeg" /></p>
<p><img alt="img" src="https://mmbiz.qpic.cn/mmbiz_gif/UicQ7HgWiaUb0tcmXDKmYswp4jknLc0ial4EcIVHXbWTowtwEXydEo170LxwemlYGC64suVWgc6KVk4P4x2J0icoWA/640?wx_fmt=gif" /></p>
<p>仿生物细胞群体机器人，其主要突破性研究成果包括：</p>
<ul>
<li>该“粒子机器人”系统可以实现鲁棒的运动和物体移动，以及光导向运动和避障；</li>
<li>与已有的仿生机器人系统相比具有更高的可扩展性和鲁棒性；</li>
<li>证明了随机性为开发具有鲁棒的确定性行为大规模群体机器人系统提供了一种有希望的方法。</li>
</ul>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190319-nvidia-jetson-nano">20190319 <a href="https://mp.weixin.qq.com/s/9ZhS5qd4wKB_Qo_OMPRqMA">NVIDIA 推出 Jetson Nano 人工智能计算机</a></h3>
<p><img alt="img" src="https://mmbiz.qpic.cn/mmbiz_jpg/VRC6xWXCPvXKTp8ggpdiahDULwwjFOQ7zP7W4mibiakFLntAjCrVoE30JLxT1ncyboibKTkzdsfqNdysd53xZF7bDg/640?wx_fmt=jpeg" /></p>
<p>这款外观小巧但功能强大的 CUDA-X™人工智能计算机为运行现代人工智能工作负载提供472 GFLOPS（每秒十亿次浮点运算）的计算性能，并且具有高能效，但耗电量仅为 5 瓦。</p>
<ul>
<li>GPU：128 核基于 NVIDIA Maxwell™架构的 GPU</li>
<li>CPU：四核 ARM®A57</li>
<li>视频：4K@30 fps（H.264/H.265）/4K@60 fps（H.264/H.265）编码和解码</li>
<li>摄像头：MIPI CSI-2 DPHY 通道，12 个模块和 1 个开发工具包</li>
<li>内存：4 GB 64 位 LPDDR4；25.6 GB/秒</li>
<li>连接性：千兆以太网</li>
<li>操作系统支持：面向 Tegra®的 Linux</li>
<li>模块尺寸：70mm x 45mm • 开发者套件尺寸：100mm x 80mm </li>
</ul>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190309">20190309 <a href="https://mp.weixin.qq.com/s/Wqog47SzPuNkhEmSfHAScA">驯服自动驾驶的长尾挑战</a></h3>
<p><a href="https://youtu.be/Q0nGo2-y0xY">YouTube 链接</a></p>
<p>经典MIT的Deep Learning for Self-driving Car课程上，邀请到了Waymo首席科学家Drago Anguelov，分享题为“Taming The Long Tail of Autonomous Driving Challenges（驯服自动驾驶的长尾挑战）”，主要是讲在现实世界中的Long Tail现象，各种异常情况该如何收集、融合、发布和测试.</p>
<ol>
<li>题目是“长尾”处理；</li>
<li>可以处理道路维修场景；</li>
<li>可以识别特殊车辆（警车/救护车/消防车）；</li>
<li>可以预防闯红灯的车辆；</li>
<li>可以对马路自行车行为轨迹预测；</li>
<li>通过NAS学习模型；</li>
<li>不完全依赖机器学习，可以利用专家知识（domain knowledge）；</li>
<li>不是E2E学习驾驶行为，而是Mid-2-Mid，就是最近的ChauffeurNet；</li>
<li>学习的行为预测有自适应性，比如激进的或者温和礼貌的；</li>
<li>仿真不能解决所有问题，仿真系统需要更多的agent model，要smart。</li>
</ol>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190305-mit-mini-cheetah-bot">20190305 <a href="https://mp.weixin.qq.com/s/HgW-qvLn0-jmMNIlr5NyrQ">MIT Mini Cheetah Bot</a></h3>
<p>MIT最新公开的新型迷你猎豹机器人是第一个能做后空翻的四足机器人。这只灵活的小豹只有20磅重，四条腿可以自然地弯曲和摆动，它还能在崎岖不平的地面上小跑，速度大约是普通人步行速度的2倍。</p>
<p>在这次升级中，猎豹被有意设计成<strong>不需依赖摄像头或任何外部传感器就能完成所有这些任务</strong>。它能灵活地 “感觉” 周围的环境，工程师们称之为 <strong>“盲眼运动”（blind locomotion）</strong>，就像人能穿过黑漆漆的房间一样。</p>
<p>猎豹 3 能够无需视觉地爬上楼梯，穿过崎岖不平的地形，并且在遇到意想不到的外力时能够迅速恢复平衡，这都要归功两种新算法：<strong>接触检测算法（contact detection algorithm）</strong>和<strong>模型预测控制算法（model-predictive control algorithm）</strong>。接触检测算法帮助机器人确定某只腿从在空中摆动切换到踏上地面的最佳时刻。例如，如果机器人踩在一根细细的树枝上，而不是踩在坚硬沉重的石头上，它会采取怎样的反应 —— 是继续迈着步子走过去，还是向后退屈一下腿 —— 可以决定它是否能保持平衡。猎豹 3 的这种无需视力的运动能力也部分归功于模型预测控制算法，该算法可以预测某条腿在踏出一步后应该施加多大的力。当任何一只腿接触到地面并施加了特定大小的力，模型预测控制算法会马上计算在未来的半秒内机器人的身体和腿应该处于什么位置。</p>
<p><img alt="minicheeta" src="../fig/minicheeta.gif" /></p>
<p><img alt="minicheeta2" src="../fig/minicheeta2.gif" /></p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190304-mit-tech-review-2019">20190304 <a href="https://mp.weixin.qq.com/s/CrPjQQlSy4FcYJ-m4FN5ww">MIT Tech Review 2019年“全球十大突破性技术”</a></h3>
<p>原文<a href="https://www.technologyreview.com/lists/technologies/2019/">链接</a></p>
<p>2019 年《麻省理工科技评论》全球十大突破性技术榜单包括：灵巧机器人、核能新浪潮、早产预测、肠道显微胶囊、定制癌症疫苗、人造肉汉堡、捕获二氧化碳、可穿戴心电仪、无下水道卫生间、流利对话的AI助手共 10 大突破性技术。</p>
<p><strong>Robot Dexterity</strong>: 重大意义：机器正在通过自我学习学会应对这个现实世界。如果机器人能学会应对混乱的现实世界，那么它们就可以胜任更多的任务。主要研究者：OpenAI（人工智能非营利组织）、卡内基梅隆大学、密歇根大学、加州大学伯克利分校 成熟期：3-5年</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190228-2018">20190228 <a href="https://mp.weixin.qq.com/s/yZ7LsGRuX49n9gPaqHoYGA">中国十大2018年度科学进展</a></h3>
<p>2019年2月27日，科技部基础研究管理中心召开“2018年度中国科学十大进展专家解读会”，发布了2018年度中国科学十大进展，以下10项重大科学进展入选：</p>
<ol>
<li>
<p>基于体细胞核移植技术成功克隆出猕猴</p>
</li>
<li>
<p>创建出首例人造单染色体真核细胞</p>
</li>
<li>
<p>揭示抑郁发生及氯胺酮快速抗抑郁机制</p>
</li>
<li>
<p>研制出用于肿瘤治疗的智能型DNA纳米机器人</p>
</li>
<li>
<p>测得迄今最高精度的引力常数G值</p>
</li>
<li>
<p>首次直接探测到电子宇宙射线能谱在1TeV附近的拐折</p>
</li>
<li>
<p>揭示水合离子的原子结构和幻数效应</p>
</li>
<li>
<p>创建出可探测细胞内结构相互作用的纳米和毫秒尺度成像技术</p>
</li>
<li>
<p>调控植物生长-代谢平衡实现可持续农业发展</p>
</li>
<li>
<p>将人类生活在黄土高原的历史推前至距今212万年</p>
</li>
</ol>
<p>“中国科学十大进展”遴选活动由科技部基础研究管理中心牵头举办，至今已成功举办14届，旨在宣传我国重大基础研究科学进展，激励广大科技工作者的科学热情和奉献精神，开展基础研究科普宣传，促进公众理解、关心和支持基础研究，在全社会营造良好的科学氛围。</p>
<p>中国科学十大进展遴选程序分为推荐、初选和终选3个环节。《中国基础科学》《科技导报》《中国科学院院刊》《中国科学基金》和《科学通报》5家编辑部推荐了353项科学研究进展，所推荐的科学进展须是在2017年12月1日至2018年11月30日期间正式发表的研究成果。</p>
<p>2018年12月，科技部基础研究管理中心组织召开了中国科学十大进展初选会议，按照推荐科学进展的学科分布，分成数理和天文科学、化学和材料科学、地球和环境科学、生命和医学科学等4个组，邀请专家从推荐的科学进展中遴选出30项进入终选。终选采取网上投票方式，邀请中国科学院院士、中国工程院院士、973计划顾问组和咨询组专家、973计划项目首席科学家、国家重点实验室主任、部分国家重点研发计划负责人等2600余名专家学者对30项候选科学进展进行网上投票，得票数排名前10 位的科学进展入选“2018年度中国科学十大进展”。</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190221-learning-agile-and-dynamic-motor-skills-for-legged-robots">20190221 <a href="https://mp.weixin.qq.com/s/xSODAGf3QcJ3A9oq6xP11A">Learning agile and dynamic motor skills for legged robots</a></h3>
<p><strong>摘要</strong>：足式机器人是机器人学中最具挑战性的主题之一。动物动态、敏捷的动作是无法用现有人为方法模仿的。一种引人注目的方法是强化学习，它只需要极少的手工设计，能够促进控制策略的自然演化。然而，截至目前，足式机器人领域的强化学习研究还主要局限于模仿，只有少数相对简单的例子被部署到真实环境系统中。主要原因在于，使用真实的机器人（尤其是使用带有动态平衡系统的真实机器人）进行训练既复杂又昂贵。本文介绍了一种<strong>可以在模拟中训练神经网络策略并将其迁移到当前最先进足式机器人系统中的方法，因此利用了快速、自动化、成本合算的数据生成方案</strong>。该方法被应用到 ANYmal 机器人中，这是一款中型犬大小的四足复杂机器人系统。利用在模拟中训练的策略，ANYmal 获得了之前方法无法实现的运动技能：它能精确、高效地服从高水平身体速度指令，奔跑速度比之前的机器人更快，甚至在复杂的环境中还能跌倒后爬起来。</p>
<p><img alt="learningAgile" src="../fig/learningAgile.JPG" /></p>
<p>图 1：创建一个控制策略。第一步是确定机器人的物理参数并估计其中的不确定性。第二步是训练一个致动器网络，建模复杂的致动器/软件动力机制。第三步是利用前两步中得到的模型训练一个控制策略。第四步是直接在物理系统中部署训练好的策略.</p>
<p><a href="#edge">Back to Top</a></p>
<h3 id="20190219-task-agnostic-self-modelling-machines">20190219 <a href="https://mp.weixin.qq.com/s/wiJ_6ssENz0kmHJSXq5utg">Task Agnostic Self-modelling Machines</a></h3>
<p>这项成果来自美国哥伦比亚大学团队，在经过35小时训练后，机器人创建了一套自我模拟，并利用自模拟器来考虑和适应不同情况，处理新任务，甚至能检测并修复机体损伤。这项成果于近日发表在Science Robotics上。</p>
<p>先制造了一个没有搭载复杂计算机结构、没有参照任何物理学、几何动力学相关知识的、与人体手臂大小相当的“铁手臂”，它有四个可以自由调节的关节臂，然后让这个“铁手臂”开始了长达35小时的随意运动……在运动的过程中，它需要收集大约1000个运动轨迹，每个轨迹包含100个运动节点，然后利用深度学习构建自我模型。在实验中，在允许机械臂根据运动轨迹自我调整的“可校准”模式下，它能以100%的成功率将多个小球夹起放入杯中。不仅如此，这个机器人还能检测自我损伤，通过自我调节后，继续处理任务。</p>
<p><img alt="taskAgnostic" src="../fig/taskAgnostic.gif" /></p>
<p><a href="#edge">Back to Top</a></p>
              
            </div>
          </div>
          <footer>
  

  <hr/>


  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright 2019, Jianan, all rights reserved.</p>
    
  </div>

  Powered by Jimmmy's Studio.
  <!-- Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
  -->
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Strategy/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
